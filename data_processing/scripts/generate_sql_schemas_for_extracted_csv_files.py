import pandas as pd
import sys
import os
import json

def generate_materialized_name(folder_name, csv_name, state_lookup, national_lookup):
    type_char = folder_name.split("_")[1][0].lower()
    folder_code = folder_name.split("_")[1][1:].upper()
    human_readable_name = "individual_people" if type_char == "p" else "housing_units"

    if folder_code == "US":
        csv_code = csv_name.split("_")[1][1:].upper()
        name = national_lookup.get(csv_code, "Unknown national code")
    elif len(folder_code) == 2:
        name = state_lookup.get(folder_code, "Unknown state code")
    else:
        raise ValueError(f"Invalid code: {folder_code}")
    
    return f"{human_readable_name}_{name.replace(' ', '_')}".lower()

if len(sys.argv) < 3:
    print("Usage: python script.py <parquet_database_path> <PUMS_data_dictionary_path>")
    sys.exit(1)

parquet_database_path, data_dictionary_path = sys.argv[1:3]

with open(data_dictionary_path, "r") as json_file:
    data_dict = json.load(json_file)

state_lookup = {code: name for name, code in [x.split("/") for x in data_dict["ST"]["Values"].values()]}
national_lookup = {"USA": "United States first tranche", "USB": "United States second tranche"}

df_csv_paths = pd.read_parquet(parquet_database_path)
models_dir = "models/public_use_microdata_sample/generated"
os.makedirs(models_dir, exist_ok=True)

for csv_path in df_csv_paths["csv_path"]:
    folder_name = os.path.basename(os.path.dirname(csv_path))
    csv_name = os.path.basename(csv_path).replace(".csv", "")
    materialized_name = generate_materialized_name(folder_name, csv_name, state_lookup, national_lookup)

    df_headers = pd.read_csv(csv_path, nrows=0)
    enum_creation_statements = []
    sql_select_parts = []

    for header in df_headers.columns:
        if header in data_dict and "Values" in data_dict[header]:
            enum_values = [f"'{x}'" for x in data_dict[header]["Values"].values()]
            enum_name = f"{header}_enum"
            enum_creation_statements.append(f"CREATE TYPE {enum_name} AS ENUM ({','.join(enum_values)});")
            mapped_column = f"CAST({header} AS {enum_name}) AS \"{data_dict[header]['Description']}\""
            sql_select_parts.append(f"    {mapped_column}")
        elif header in data_dict:
            description = data_dict[header]["Description"].replace("'", "''")
            sql_select_parts.append(f"    {header} AS \"{description}\"")
        else:
            sql_select_parts.append(f"    {header}")

    sql_select_statement = ",\n".join(sql_select_parts)

    sql_content = f"""-- SQL transformation for {csv_name} generated by {os.path.basename(__file__)}
{'\n'.join(enum_creation_statements)}

{{{{ config(materialized='external', location=var('output_path') + '/{materialized_name}.parquet') }}}}
SELECT
{sql_select_statement}
FROM read_csv_auto('{csv_path}');
"""

    sql_file_path = os.path.join(models_dir, f"{materialized_name}.sql")
    with open(sql_file_path, "w") as sql_file:
        sql_file.write(sql_content)
